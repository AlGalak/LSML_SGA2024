{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-10-27 12:49:17,423 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = se.read.csv(\"/data/clickstream.csv\", header=True, sep='\\t')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DF solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|route                |count|\n",
      "+---------------------+-----+\n",
      "|main                 |8184 |\n",
      "|main-archive         |1113 |\n",
      "|main-rabota          |1047 |\n",
      "|main-internet        |897  |\n",
      "|main-bonus           |870  |\n",
      "|main-news            |769  |\n",
      "|main-tariffs         |676  |\n",
      "|main-online          |587  |\n",
      "|main-vklad           |518  |\n",
      "|main-rabota-archive  |170  |\n",
      "|main-archive-rabota  |167  |\n",
      "|main-bonus-archive   |143  |\n",
      "|main-rabota-bonus    |139  |\n",
      "|main-bonus-rabota    |135  |\n",
      "|main-news-rabota     |135  |\n",
      "|main-archive-internet|132  |\n",
      "|main-rabota-news     |130  |\n",
      "|main-internet-rabota |129  |\n",
      "|main-archive-news    |126  |\n",
      "|main-rabota-internet |124  |\n",
      "|main-internet-archive|123  |\n",
      "|main-archive-bonus   |117  |\n",
      "|main-tariffs-internet|114  |\n",
      "|main-internet-bonus  |114  |\n",
      "|main-news-archive    |113  |\n",
      "|main-news-internet   |109  |\n",
      "|main-archive-tariffs |104  |\n",
      "|main-internet-news   |103  |\n",
      "|main-tariffs-archive |103  |\n",
      "|main-rabota-main     |94   |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define window for each session ordered by timestamp\n",
    "window_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
    "\n",
    "# Identify the timestamp of the first error in each session\n",
    "df_with_error_timestamp = df.withColumn(\n",
    "    \"error_timestamp\",\n",
    "    F.min(F.when(F.col(\"event_type\").like(\"%error%\"), F.col(\"timestamp\"))).over(window_spec)\n",
    ")\n",
    "\n",
    "# Filter out events occurring after the first error in each session\n",
    "clickstream_filtered = df_with_error_timestamp.filter(\n",
    "    (F.col(\"error_timestamp\").isNull()) | (F.col(\"timestamp\") < F.col(\"error_timestamp\"))\n",
    ")\n",
    "\n",
    "# Add row numbers within each session \n",
    "sorted_df = clickstream_filtered.withColumn(\n",
    "    \"row_num\",\n",
    "    F.row_number().over(window_spec)\n",
    ").orderBy(\"user_id\", \"session_id\", \"timestamp\", \"row_num\")\n",
    "\n",
    "# Remove consecutive duplicate event pages within the same session\n",
    "deduped_df = sorted_df.withColumn(\n",
    "    \"prev_event_page\",\n",
    "    F.lag(\"event_page\").over(window_spec)\n",
    ").filter(\n",
    "    (F.col(\"prev_event_page\").isNull()) | (F.col(\"event_page\") != F.col(\"prev_event_page\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Define a UDF to remove consecutive duplicates in collected list\n",
    "def remove_consecutive_duplicates(event_pages):\n",
    "    if not event_pages:\n",
    "        return \"\"\n",
    "    deduped_pages = [event_pages[0]]\n",
    "    for page in event_pages[1:]:\n",
    "        if page != deduped_pages[-1]:\n",
    "            deduped_pages.append(page)\n",
    "    return '-'.join(deduped_pages)\n",
    "\n",
    "\n",
    "remove_duplicates_udf = F.udf(remove_consecutive_duplicates, StringType())\n",
    "\n",
    "# Group by session, remove duplicates and form routes\n",
    "routes_df = deduped_df.groupBy(\"user_id\", \"session_id\").agg(\n",
    "    remove_duplicates_udf(F.collect_list(\"event_page\")).alias(\"route\")\n",
    ")\n",
    "\n",
    "# Count occurrences of each route and sort in descending order\n",
    "route_counts = routes_df.groupBy(\"route\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "top_30_routes = route_counts.limit(30)\n",
    "\n",
    "top_30_routes.show(top_30_routes.count(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:============================================>           (11 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|route                |count|\n",
      "+---------------------+-----+\n",
      "|main                 |8184 |\n",
      "|main-archive         |1112 |\n",
      "|main-rabota          |1047 |\n",
      "|main-internet        |897  |\n",
      "|main-bonus           |870  |\n",
      "|main-news            |769  |\n",
      "|main-tariffs         |677  |\n",
      "|main-online          |587  |\n",
      "|main-vklad           |517  |\n",
      "|main-rabota-archive  |170  |\n",
      "|main-archive-rabota  |167  |\n",
      "|main-bonus-archive   |143  |\n",
      "|main-rabota-bonus    |139  |\n",
      "|main-bonus-rabota    |135  |\n",
      "|main-news-rabota     |135  |\n",
      "|main-archive-internet|131  |\n",
      "|main-internet-rabota |129  |\n",
      "|main-rabota-news     |129  |\n",
      "|main-archive-news    |126  |\n",
      "|main-rabota-internet |124  |\n",
      "|main-internet-archive|123  |\n",
      "|main-archive-bonus   |117  |\n",
      "|main-internet-bonus  |115  |\n",
      "|main-tariffs-internet|114  |\n",
      "|main-news-archive    |113  |\n",
      "|main-news-internet   |109  |\n",
      "|main-archive-tariffs |104  |\n",
      "|main-internet-news   |103  |\n",
      "|main-tariffs-archive |103  |\n",
      "|main-rabota-main     |94   |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view for SQL querying\n",
    "df.createOrReplaceTempView(\"clickstream\")\n",
    "\n",
    "sql_query = \"\"\"\n",
    "WITH deduped_events AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        timestamp,\n",
    "        event_page,\n",
    "        event_type,\n",
    "        MIN(CASE WHEN event_type LIKE '%error%' THEN timestamp END) OVER (PARTITION BY user_id, session_id) AS error_timestamp,\n",
    "        LAG(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS prev_event_page\n",
    "    FROM clickstream\n",
    "),\n",
    "filtered_events AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        event_page\n",
    "    FROM deduped_events\n",
    "    WHERE (error_timestamp IS NULL OR timestamp < error_timestamp)\n",
    "      AND (prev_event_page IS NULL OR event_page != prev_event_page)\n",
    "),\n",
    "routes AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        CONCAT_WS('-', COLLECT_LIST(event_page)) AS route\n",
    "    FROM filtered_events\n",
    "    GROUP BY user_id, session_id\n",
    "),\n",
    "route_counts AS (\n",
    "    SELECT\n",
    "        route,\n",
    "        COUNT(*) AS count\n",
    "    FROM routes\n",
    "    GROUP BY route\n",
    ")\n",
    "SELECT route, count\n",
    "FROM route_counts\n",
    "ORDER BY count DESC\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "top_30_routes = se.sql(sql_query)\n",
    "\n",
    "top_30_routes.show(top_30_routes.count(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:======================================>                 (22 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: 8185\n",
      "main-archive: 1113\n",
      "main-rabota: 1047\n",
      "main-internet: 897\n",
      "main-bonus: 870\n",
      "main-news: 769\n",
      "main-tariffs: 677\n",
      "main-online: 587\n",
      "main-vklad: 518\n",
      "main-rabota-archive: 170\n",
      "main-archive-rabota: 167\n",
      "main-bonus-archive: 143\n",
      "main-rabota-bonus: 139\n",
      "main-bonus-rabota: 135\n",
      "main-news-rabota: 135\n",
      "main-archive-internet: 132\n",
      "main-rabota-news: 130\n",
      "main-internet-rabota: 129\n",
      "main-archive-news: 126\n",
      "main-rabota-internet: 124\n",
      "main-internet-archive: 123\n",
      "main-archive-bonus: 117\n",
      "main-internet-bonus: 115\n",
      "main-news-archive: 113\n",
      "main-tariffs-internet: 113\n",
      "main-news-internet: 109\n",
      "main-archive-tariffs: 104\n",
      "main-internet-news: 103\n",
      "main-tariffs-archive: 103\n",
      "main-rabota-main: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "rdd = sc.textFile(\"/data/clickstream.csv\")\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header).map(lambda row: row.split('\\t'))\n",
    "\n",
    "data_rdd = data_rdd.map(lambda row: (row[0], row[1], row[2], row[3], row[4]))\n",
    "\n",
    "# Filter rows with errors and mark events after error in each session\n",
    "def filter_errors(session_data):\n",
    "    clean_session = []\n",
    "    error_occurred = False\n",
    "    for row in session_data:\n",
    "        if 'error' in row[2]:  # Check if event_type contains 'error'\n",
    "            error_occurred = True\n",
    "        if not error_occurred:\n",
    "            clean_session.append(row)\n",
    "    return clean_session\n",
    "\n",
    "# Group by user and session, then filter out events after the first error\n",
    "filtered_rdd = data_rdd.map(lambda x: ((x[0], x[1]), x)) \\\n",
    "                       .groupByKey() \\\n",
    "                       .flatMapValues(filter_errors) \\\n",
    "                       .map(lambda x: x[1])\n",
    "\n",
    "# Sort events by timestamp within each session\n",
    "sorted_rdd = filtered_rdd.map(lambda x: ((x[0], x[1]), (x[3], x[4]))) \\\n",
    "                         .groupByKey() \\\n",
    "                         .mapValues(lambda pages: sorted(enumerate(pages), key=lambda x: (x[1][1], x[0]))) \\\n",
    "                         .mapValues(lambda pages: [page[1][0] for page in pages])  # Extract event_page from sorted tuples\n",
    "\n",
    "\n",
    "# Remove consecutive duplicate pages within each session\n",
    "def remove_consecutive_duplicates(event_pages):\n",
    "    deduped_pages = [event_pages[0]]\n",
    "    for page in event_pages[1:]:\n",
    "        if page != deduped_pages[-1]:\n",
    "            deduped_pages.append(page)\n",
    "    return deduped_pages\n",
    "\n",
    "\n",
    "routes_rdd = sorted_rdd.mapValues(remove_consecutive_duplicates) \\\n",
    "                       .mapValues(lambda pages: '-'.join(pages)) \\\n",
    "                       .map(lambda x: x[1])\n",
    "\n",
    "# Count occurrences of each route and sort in descending order\n",
    "route_counts = routes_rdd.map(lambda route: (route, 1)) \\\n",
    "                         .reduceByKey(add) \\\n",
    "                         .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "top_30_routes = route_counts.take(30)\n",
    "\n",
    "for route, count in top_30_routes:\n",
    "    print(f\"{route}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "routes_dict = {route: count for route, count in top_30_routes}\n",
    "\n",
    "with open(\"result_top30.json\", \"w\") as json_file:\n",
    "    json.dump(routes_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
